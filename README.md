## Overview

The Neural Latents Benchmark (NLB) aims to evaluate models of neural population state. In the first benchmark suite, participating models should take multi-channel spiking activity as input and produce firing rate estimates as output. **This first benchmark will be released in March 2021**.

The first benchmark suite will consist of a collection of 3-4 datasets of neural spiking activity and a public challenge hosted on [EvalAI](https://eval.ai/). While the benchmark will be available indefinitely, the challenge will close and announce winners in Fall 2021.

### Pointers
- For updates regarding the benchmark, please subscribe to [the mailing list](https://forms.gle/o7BejfJ2S9hqJpM28).

### FAQ
#### How do I submit a model to the benchmark?
**Please note, the first benchmark will open in March 2021.** We are hosting our challenge on [EvalAI](https://eval.ai/), a platform for evaluating AI models. On the platform, you can choose to make private or public submissions to any or all of the individual datasets.

#### Can I view the leaderboard without submitting?
Yes, the full leaderboard will be available on this website indefinitely (courtesy of EvalAI), and EvalAI is also synced with [Papers With Code](https://paperswithcode.com/).

#### Is there a deadline?
The benchmark and its leaderboard can be submittted to indefinitely on EvalAI as a resource for the community. However, the winners of the challenge will be determined from the leaderboard in Fall 2021. Prizes for the challenge will be distributed to the winner then.

#### Is NLB one benchmark or many benchmarks?
NLB aims to regularly organize benchmark suites, a collection of tasks, datasets, and metrics around a theme in neural latent variable modeling. For example, the first benchmark suite will emphasize general population modeling.