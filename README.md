<!-- <div style="margin-bottom:1em"> -->
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/o7dvFLHb5AY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
<!-- </div> -->
## Overview

Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external
experimental variables. However, progress in latent variable modeling is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce the Neural Latents Benchmark (NLB). In the first benchmark suite, NLB 2021, participating models are evaluated on 7 datasets of neural spiking activity spanning 4 tasks and brain areas.
<!-- Models should take multi-channel spiking activity as input and produce firing rate estimates as output. Rate estimates should then be submitted to the public challenge hosted on [EvalAI](https://eval.ai/).  -->
While the benchmark will be available indefinitely, the challenge will close March 6, 2022. To get started with the challenge, follow the links on the left.

- [Read about NLB 2021 in our technical paper](http://arxiv.org/abs/2109.04463).
- [Join the mailing list for updates](https://forms.gle/o7BejfJ2S9hqJpM28).
- [See our Cosyne '21 announcement](https://www.youtube.com/watch?v=o7dvFLHb5AY).
- [Join our Slack workspace](https://neurallatents.slack.com). Please email `fpei6 [at] gatech [dot] edu` for an invite link.

## FAQ
### How do I submit a model to the benchmark?
We are hosting our challenge on [EvalAI](https://eval.ai/web/challenges/challenge-page/1256/overview), a platform for evaluating machine learning models. On the platform, you can choose to make private or public submissions to any or all of the individual datasets.

### Can I view the leaderboard without submitting?
Yes, the full leaderboard will be available on [EvalAI](https://eval.ai/web/challenges/challenge-page/1256/leaderboard), and EvalAI is also synced with [Papers With Code](https://paperswithcode.com/). Model open-sourcing is encouraged and thus may be available through the leaderboard.

### Is there a deadline?
The benchmark and its leaderboard can be submitted to indefinitely on EvalAI as a resource for the community. However, the winners of the challenge will be determined from the leaderboard on March 6th, 2022. Prizes for the challenge will be distributed to the winner shortly thereafter.

### Is NLB one benchmark or many benchmarks?
NLB aims to regularly organize benchmark suites, a collection of tasks, datasets, and metrics around a theme in neural latent variable modeling. For example, NLB'21 will emphasize general population modeling.

## Contact
The Neural Latents Benchmark is being led by the Systems Neural Engineering Lab in collaboration with labs across several universities. General inquiries should be directed to [Dr. Pandarinath] at `chethan [at] gatech [dot] edu`.
