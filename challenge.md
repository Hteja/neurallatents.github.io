---
title: Challenge Guidelines
layout: default
filename: challenge.md
---

## Challenge Guidelines

As part of the benchmarking effort, we are hosting a competition and offering prizes for the best-performing submissions on the EvalAI leaderboard on the deadline of **January 7th, 2022, anywhere on Earth**.
 <!-- and will be funded by the SNEL Lab at Emory. -->

### Ranking and Prizes

The challenge evaluates **co-smoothing** performance on individual datasets and across all datasets. The overall winner will be chosen by average rank across datasets. For the MC_Maze scaling datasets, each dataset session will be weighted as 1/3 of a dataset in both individual and overall ranking calculations.

Prizes will be provided in the form of Visa gift cards.
- A prize of 3,000 USD will be given for best average rank across tasks.
- A prize of 1,000 USD will be given for best rank on each individual task (and best average rank on 3 scaling datasets).

### Eligibility

To be considered for challenge prizes, teams must submit their methods to the EvalAI challenge before the deadline. After the deadline, we will notify top contenders of their tentative rankings. We reserve the right to have participants share their code and scripts to reproduce their results with us, so that we can validate submission fairness. After this validation phase, we will announce the winners.

In general, we recommend participants to publicly share their code to help promote progress in the field.


