## Overview

The Unsupervised Modeling of Brain Activity (UMBrA) benchmark aims to evaluate models of neural population state. Participating models should take multi-channel spiking activity as input and produce firing rate estimates as output. **The benchmark will be released in March 2021**.

In this first iteration of the benchmark, the benchmark will consist of a collection of 3-4 datasets of neural spiking activity and a public challenge hosted on [EvalAI](https://eval.ai/). While the benchmark will be available indefinitely, the challenge will close and announce winners in Fall 2021. 

### Pointers
- For updates regarding the benchmark, please subscribe to [the mailing list](https://forms.gle/o7BejfJ2S9hqJpM28).

<!-- 
- For details on model submission, datasets, and starter code, visit [the GitHub page](https://github.com/snel-repo/neural-data-benchmark/).
- For general inquiries, please open a [Github issue](https://github.com/snel-repo/neural-data-benchmark/issues).
- The forthcoming motivating paper will describe benchmark motivation and design.
- The forthcoming challenge and leaderboard are available on EvalAI.
-->

### FAQ
#### How do I submit a model to the benchmark?
**Please note, the benchmark will open in March 2021.** We are hosting our challenge on [EvalAI](https://eval.ai/), a platform for evaluating AI models. On the platform, you can choose to make private or public submissions to any or all of the individual datasets. 

#### Can I view the leaderboard without submitting?
Yes, the full leaderboard will be available on this website indefinitely (courtesy of EvalAI), and EvalAI is also synced with [Papers With Code](https://paperswithcode.com/).

#### Is there a deadline?
The benchmark and its leaderboard can be submittted to indefinitely on EvalAI as a resource for the community. However, the winners of the challenge will be determined from the leaderboard in Fall 2021. Prizes for the challenge will be distributed to the winner then.

<!--
#### What information do I need to participate?
In a submission, you should provide:
- A name for your method, displayed in the challenge
- Your method's predictions on the test set data
- A brief description of your method. Please include external resources e.g. training data if used.
- A link to a paper and/or a code repository for your method.

#### How should I cite this benchmark?
We will release a motivational paper and provide its citation soon.
-->
